\chapter{Background}\label{chap:background}
Some background knowledge is necessary for the reader to have an understanding of the principles behind semantic segmentation and convolutional neural networks.
This chapter on the background of this work will discuss semantic segmentation, machine learning with artificial neural networks and convolutional neural networks, curb segmentation, loss functions, network optimizers, and binary dilation.

\section{Semantic Segmentation}\label{section:background-segmentation}
Unlike image classification, which classifies the contents of an image as a whole, semantic segmentation is the use of some algorithm to process an image and assign class labels to each individual pixel~\cite{segmentation-medium}.
This adds the ability to locate and classify multiple objects in a given scene.
For example, the ground truth segmentation of the street-level image in \figref{fig:background-raw} can be seen in \figref{fig:background-segmented}.
Each pixel of the image has been assigned a class, which is represented by the different colors in the segmented image.
This allows a computer or program to understand what objects are in the image it is shown.

\input{figures/background/segmentation}

By segmenting an image in this way, the program can interpret the scene semantically.
For example, by receiving the segmented image, a program can identify that there are line markings on the road and that there is a vehicle in front of it.
The segmentation of images in this way is essential in many robotic applications as it allows further higher level processing of the scene.

Towards the goal of this work, the segmentation of traversability classes in a scene allows Obelix to more accurately find a path allowing for the safe traversal from sidewalk to street level via curb cuts.

\input{chapters/3_1-neural-networks}

\section{Curbs and Curb Cuts}\label{section:background-curbs}
Curbs are the concrete or stone edging of a road. Curbs usually separate the road from some other area with a different elevation, usually a pedestrian sidewalk.
An example of a sidewalk with a curb can be seen in Figure \ref{fig:background-curb}.
Curb cuts are "cuts" in the curb that allow a pedestrian sidewalk to have a gentle slope down to street level.
An example of a sidewalk with a curb cut can be seen in Figure \ref{fig:background-curbcut}.
Originally, these cuts were made to allow accessibility access, especially for those requiring wheelchairs.
To a wheelchair user, curbs represent a significant barrier in terms of traversability, as was discussed in the article "Curb Cuts" by Cynthia Gorney and Delaney Hall \cite{99pi}. 
Curb cuts first started appearing fifty years ago from the efforts of activist Ed Roberts and his push to make city streets more accessible.

Wheeled robots are also unable to traverse curbs.
They therefore require curb cuts to traverse urban environments where curbs are present.

\input{figures/background/curbs}

\section{Loss Functions}\label{section:background-loss}
The loss function $\ell$ maps the output of a neural network $\hat{y}$ and the target $y$ onto a real number and represents the "cost" of an output.
The goal of learning is to minimize this cost value.
Effectively, the loss function measures the difference between the predicted value and the target.
In our case, the target value is the ground truth labeling of an image and the output is the softmax class predictions from the network.
There are many different loss functions that are commonly used. For the purposes of image segmentation, the most commonly used function is cross entropy loss.

\subsection{Cross Entropy Loss}\label{section:background-crossentropy}
The cross entropy loss is formally defined as:
\begin{align}
	\ell(y, \hat{y}) &=\sum_{m}-y_m\log(\hat{y}_m)
\end{align}
where $m$ is the class, $y_m$ the ground truth value for a class $m$, and $\hat{y}_m$ the softmax prediction of a class $m$ by the model. 
As such, cross entropy loss calculates the error of the model to classify a certain value correctly.
The loss of two predictions could be the same, as long as the prediction for the true class, i.e. the class that has value 1 in $y$, remains constant.
The loss is thus independent of how the probability is split between the remaining classes.

For example, let the ground truth classification be class 0, i.e. $y = \{1, 0, 0\}$ with one-hot encoding.
Let the current model prediction be $\hat{y} = \{0.5, 0.2, 0.3\}$, also with one-hot encoding.
The loss would then be:
\begin{equation}
	\begin{split}
		\ell(y, \hat{y}) 	&= \sum_{m}-y_m \log(\hat{y}_m) \\
		&= -(1)\log(0.5) + (-0)\log(0.2) + (-0)\log(0.3)\\
		&= -\log(0.5) \\
		&= 0.301...
	\end{split}
\end{equation}
Alternatively, if model prediction is exactly the same as the ground truth, then we would have:
\begin{equation}
	\begin{split}
		y					&= \hat{y} = \{1, 0, 0\}\\
		\ell(y, \hat{y}) 	&= \sum_{m}-y_m \log(\hat{y}_m) \\
		&= -(1)\log(1) + (-0)\log(0) + (-0)\log(0)\\
		&= -\log(1) \\
		&= 0
	\end{split}
\end{equation}
Applying this on the full image, this becomes:
\begin{align}
	\ell(y, \hat{y}) &=\sum_u \sum_v \sum_{m}-y_{m, u, v}\log(\hat{y}_{m,u,v})
\end{align}
where $u$ and $v$ are the pixel coordinates.

The weighted variant, known simply as weighted cross entropy loss, is defined as:
\begin{align}
	\ell(y, \hat{y}) &=\sum_{m}-y_m\log(\hat{y}_m) \cdot d_m
\end{align}
where $d_m$ is the weight for a given class $m$.

Using the weighted version allows changes to how much each class contributes to the loss.
This is done to counteract class imbalance that could be caused by having an imbalance in the distribution or frequency of each class in the dataset.

\section{Optimizers}\label{section:background-optimizers}
During training, network parameters must be updated to minimize the loss function at each iteration.
This is done by the optimizer.
The optimizer updates the network parameters in such a way as to minimize the loss function~\cite{optimizer-intro}.

Gradient Descent is one of the earliest optimizers.
It works by calculating what a small change in each network parameter would do to the loss function, i.e. determines the gradient of the loss for the current iteration.
It then adjusts the network parameters according to this gradient.
This process is repeated at each iteration to further minimize the loss function.
This process can be time-consuming, especially with larger datasets.
Stochastic Gradient Descent (SGD) is a more commonly used variant of gradient descent and works similarly, but working only on randomly selected batches of the training data at each iteration~\cite{optimizer-intro}.
Due to the large size of datasets, training is usually done in batches.
By using SGD, the optimization step can occur after processing every batch, rather than after the entire dataset is processed.
This speeds up with respect to wall-clock time network training.

Adaptive Moment Estimation (Adam) is another optimizer that is commonly used and first proposed in the paper "Adam: A method for stochastic optimization" by Diederik Kingma and Jimmy Ba~\cite{adam}.
It combines concepts from Adaptive Gradient Algorithm (AdaGrad), which has per-parameter learning rates, and Root Mean Square Propagation (RMSProp), whose learning rates are adapted based on the average of the magnitude of recent gradients~\cite{adam}.
Adam utilizes the concept of momentum, which incorporates previous gradients into the current one.
It does this by calculating an exponential moving average and the square of previous gradients and using these to determine the next gradient.

\section{Backpropagation}\label{section:background-backpropagation}
Backpropagation is a learning procedure used in artificial neural networks to adjust network weights and produce internal representations of "important features in the task domain," first described in 1986 by David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams in their paper "Learning representations by back-propagating errors"~\cite{backprop}.
Backpropagation efficiently and repeatedly adjusts the network weights to minimize the network error, which is calculated by the loss function.

Calculating the network weights is done layer by layer, starting with the final output layer.
The partial derivative of $\ell$ with respect to $y_j$, intuitively the effect the output unit has on the loss, is defined as:
\begin{align}\label{eq:loss-1}
	\frac{\partial \ell}{\partial y_j}
\end{align}
where $y_j$ is the output of a single unit in the final layer.
The contribution of the input of the unit $j$ on the error can then be calculated using the value previously calculated in \eqref{eq:loss-1} using the chain rule for differentiation as
\begin{align}\label{eq:loss-2}
	\frac{\partial\ell}{\partial x_j} &= \frac{\partial \ell}{\partial y_j} \cdot \frac{\partial y_j}{\partial x_j}
\end{align}
We now have a description of how changing the input of unit $j$ will affect the loss.
As such, we can then also represent how changing the weight $w_{ji}$ of a connection between the unit $j$ and a unit in the previous layer $i$ will effect the error as
\begin{align}\label{eq:loss-3}
	\frac{\partial \ell}{\partial w_{ji}} &= \frac{\partial \ell}{\partial x_j} \cdot \frac{\partial x_j}{\partial w_{ji}}
\end{align}
In this case, the previous layer means the layer which during forwards-propagation would be calculated immediately before the layer in which unit $j$ is.
Again, due to the use of the chain rule, the previous value calculated in \eqref{eq:loss-2} can be used here.
This gives us the contribution of the weight $w_{ji}$ on the error and, by multiplying by the learning rate, the amount by which the gradient must be changed for the next training iteration.

The error contribution of the output of unit $i$ via a single successor unit $j$ can also similarly be calculated using the value from \eqref{eq:loss-2}.
\begin{align}\label{eq:loss-4}
	\frac{\partial \ell}{\partial y_i} &= \frac{\partial \ell}{\partial x_j} \cdot w_{ji}
\end{align}
Since unit $i$ is connected to multiple successor units, its value is a summation of its output contribution via all units $j \in J$, where $J$ is the set of all units in successive layers that $i$ is connected to.
\begin{align}
	\frac{\partial \ell}{\partial y_i} &= \sum_{j \in J} \frac{\partial \ell}{\partial x_j} \cdot w_{ji}
\end{align}
This calculation can be done in parallel, thanks to parallel computing architecture, completing the gradient calculations for the penultimate layer.
This procedure is then repeated for all layers, each time using the previously computed values to allow for efficient computation.

\input{chapters/3_2-hyperparameters}

\section{Binary Dilation}\label{section:background-dilation}
Binary dilation is a basic morphological operation and is usually represented by the operator $\oplus$.
With regards to this work, binary dilation is used in our loss function.
For a given binary image viewed as an integer grid $\mathbb{Z}^d$ for some dimension $d$, let $E$ be an integer grid, $A \in E$ a binary image, and $B \in \{0,1\}^d$ a structuring element.
The binary dilation of $A$ by $B$ is then defined as:
\begin{align}
	A \oplus B &= \bigcup_{b \in B}A_b
\end{align}
where $A_b$ is the translation of $A$ by $b$.
This can be seen as extending the area of the binary image $A$ by locus of the points covered by $B$ given that $B$ has a center on the origin~\cite{morphology}.